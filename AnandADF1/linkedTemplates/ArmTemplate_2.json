{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"factoryName": {
			"type": "string",
			"metadata": "Data Factory name",
			"defaultValue": "AnandADF1"
		}
	},
	"variables": {
		"factoryId": "[concat('Microsoft.DataFactory/factories/', parameters('factoryName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('factoryName'), '/UpsertSource')]",
			"type": "Microsoft.DataFactory/factories/datasets",
			"apiVersion": "2018-06-01",
			"properties": {
				"description": "This is source Datasets for Alter Table Transformation",
				"linkedServiceName": {
					"referenceName": "anandstorage1_LinkService",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": "Employee.csv",
						"fileSystem": "container1"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": [
					{
						"name": "empId",
						"type": "String"
					},
					{
						"name": "empName",
						"type": "String"
					},
					{
						"name": "gender",
						"type": "String"
					},
					{
						"name": "deptid",
						"type": "String"
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/13 AlertMail and ScheduleTrigger')]",
			"type": "Microsoft.DataFactory/factories/pipelines",
			"apiVersion": "2018-06-01",
			"properties": {
				"activities": [
					{
						"name": "Copy data1",
						"type": "Copy",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "DelimitedTextSource",
								"storeSettings": {
									"type": "AzureBlobFSReadSettings",
									"recursive": true,
									"enablePartitionDiscovery": false
								},
								"formatSettings": {
									"type": "DelimitedTextReadSettings"
								}
							},
							"sink": {
								"type": "DelimitedTextSink",
								"storeSettings": {
									"type": "AzureBlobFSWriteSettings"
								},
								"formatSettings": {
									"type": "DelimitedTextWriteSettings",
									"quoteAllText": true,
									"fileExtension": ".txt"
								}
							},
							"enableStaging": false,
							"translator": {
								"type": "TabularTranslator",
								"typeConversion": true,
								"typeConversionSettings": {
									"allowDataTruncation": true,
									"treatBooleanAsNumber": false
								}
							}
						},
						"inputs": [
							{
								"referenceName": "ScheduleTrigger_Source",
								"type": "DatasetReference",
								"parameters": {}
							}
						],
						"outputs": [
							{
								"referenceName": "ScheduleTrigger_Destination",
								"type": "DatasetReference",
								"parameters": {}
							}
						]
					},
					{
						"name": "Web1",
						"type": "WebActivity",
						"dependsOn": [
							{
								"activity": "Copy data1",
								"dependencyConditions": [
									"Failed"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"url": "https://prod-19.centralindia.logic.azure.com:443/workflows/962756bf7f1946828d03231524bcabc2/triggers/manual/paths/invoke?api-version=2016-10-01&sp=%2Ftriggers%2Fmanual%2Frun&sv=1.0&sig=6rra35IWdi2AiXxe24PM_C-LRzSQFEsvX0YE-cFHw84",
							"method": "POST",
							"headers": {},
							"body": {
								"value": "{\n    \"To\":\"Anand.Pandian@elait.com\",\n    \"Subject\":\"Pipeline Failed Remainder\",\n    \"Body\":\"Hi Anand! Your Pipeline has Failed  \n     Please checkit with following Pipeline detail\",\n    \"Data factory name\":\"Adf Name : @{pipeline().DataFactory}\",\n    \"Pipeline Name\":\"Pipeline Name : @{pipeline().Pipeline}\",\n    \"Pipeline Run Id\":\"Pipeline RunId : @{pipeline().RunId}\",\n    \"Pipeline trigger Name\":\"Trigger Name : @{pipeline().TriggerName}\",\n    \"Pipeline trigger Name\":\"Trigger Type : @{pipeline().TriggerType}\",\n    \"Pipeline trigger time\":\"Trigger time : @{pipeline().TriggerTime}\",\n    \"Error Message\":\"Error Message : @{activity('Copy data1').output.errors[0].Message}\"\n}",
								"type": "Expression"
							}
						}
					},
					{
						"name": "Stored procedure1",
						"type": "SqlServerStoredProcedure",
						"dependsOn": [
							{
								"activity": "Copy data1",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"storedProcedureName": "[[dbo].[logdetails]",
							"storedProcedureParameters": {
								"pipelineName": {
									"value": {
										"value": "@pipeline().Pipeline",
										"type": "Expression"
									},
									"type": "String"
								},
								"pipelineRunid": {
									"value": {
										"value": "@pipeline().RunId",
										"type": "Expression"
									},
									"type": "String"
								},
								"pipelineTriggerid": {
									"value": {
										"value": "@pipeline().TriggerId",
										"type": "Expression"
									},
									"type": "String"
								},
								"pipelineTriggername": {
									"value": {
										"value": "@pipeline().TriggerName",
										"type": "Expression"
									},
									"type": "String"
								},
								"pipelineTriggertime": {
									"value": {
										"value": "@pipeline().TriggerTime",
										"type": "Expression"
									},
									"type": "String"
								},
								"pipelineTriggertype": {
									"value": {
										"value": "@pipeline().TriggerType",
										"type": "Expression"
									},
									"type": "String"
								}
							}
						},
						"linkedServiceName": {
							"referenceName": "AnandSqlDB_LinkedService",
							"type": "LinkedServiceReference"
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"annotations": [],
				"lastPublishTime": "2022-11-10T06:26:55Z"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/14 Event Trigger')]",
			"type": "Microsoft.DataFactory/factories/pipelines",
			"apiVersion": "2018-06-01",
			"properties": {
				"activities": [
					{
						"name": "Delete1",
						"type": "Delete",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataset": {
								"referenceName": "EventTrigger_Source",
								"type": "DatasetReference",
								"parameters": {}
							},
							"logStorageSettings": {
								"linkedServiceName": {
									"referenceName": "anandstorage1_LinkService",
									"type": "LinkedServiceReference"
								}
							},
							"enableLogging": true,
							"storeSettings": {
								"type": "AzureBlobFSReadSettings",
								"recursive": true,
								"enablePartitionDiscovery": false
							}
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"annotations": [],
				"lastPublishTime": "2022-11-09T03:14:42Z"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/2 Load Data')]",
			"type": "Microsoft.DataFactory/factories/pipelines",
			"apiVersion": "2018-06-01",
			"properties": {
				"description": "Load data from azure data lake storage to azure SQL database",
				"activities": [
					{
						"name": "Copy data1",
						"type": "Copy",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [
							{
								"name": "Source",
								"value": "container1//Student.csv"
							},
							{
								"name": "Destination",
								"value": "dbo.student"
							}
						],
						"typeProperties": {
							"source": {
								"type": "DelimitedTextSource",
								"storeSettings": {
									"type": "AzureBlobFSReadSettings",
									"recursive": true,
									"enablePartitionDiscovery": false
								},
								"formatSettings": {
									"type": "DelimitedTextReadSettings"
								}
							},
							"sink": {
								"type": "AzureSqlSink",
								"writeBehavior": "insert",
								"sqlWriterUseTableLock": false,
								"disableMetricsCollection": false
							},
							"enableStaging": false,
							"translator": {
								"type": "TabularTranslator",
								"mappings": [
									{
										"source": {
											"name": "stuId",
											"type": "String",
											"physicalType": "String"
										},
										"sink": {
											"name": "stuId",
											"type": "Int32",
											"physicalType": "int"
										}
									},
									{
										"source": {
											"name": "stuName",
											"type": "String",
											"physicalType": "String"
										},
										"sink": {
											"name": "stuName",
											"type": "String",
											"physicalType": "varchar"
										}
									},
									{
										"source": {
											"name": "fees",
											"type": "String",
											"physicalType": "String"
										},
										"sink": {
											"name": "fees",
											"type": "Int32",
											"physicalType": "int"
										}
									}
								],
								"typeConversion": true,
								"typeConversionSettings": {
									"allowDataTruncation": true,
									"treatBooleanAsNumber": false
								}
							}
						},
						"inputs": [
							{
								"referenceName": "LoadDataSource",
								"type": "DatasetReference",
								"parameters": {}
							}
						],
						"outputs": [
							{
								"referenceName": "LoadDataDestination",
								"type": "DatasetReference",
								"parameters": {}
							}
						]
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"annotations": [],
				"lastPublishTime": "2022-11-09T10:15:30Z"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/3 MultipleFiles Source to destination')]",
			"type": "Microsoft.DataFactory/factories/pipelines",
			"apiVersion": "2018-06-01",
			"properties": {
				"activities": [
					{
						"name": "Lookup1",
						"type": "Lookup",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "AzureSqlSource",
								"queryTimeout": "02:00:00",
								"partitionOption": "None"
							},
							"dataset": {
								"referenceName": "SQLMetaTable",
								"type": "DatasetReference",
								"parameters": {}
							},
							"firstRowOnly": false
						}
					},
					{
						"name": "ForEach1",
						"type": "ForEach",
						"dependsOn": [
							{
								"activity": "Lookup1",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@activity('Lookup1').output.value",
								"type": "Expression"
							},
							"isSequential": true,
							"activities": [
								{
									"name": "Copy data1",
									"type": "Copy",
									"dependsOn": [],
									"policy": {
										"timeout": "0.12:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"source": {
											"type": "DelimitedTextSource",
											"storeSettings": {
												"type": "AzureBlobFSReadSettings",
												"recursive": true,
												"enablePartitionDiscovery": false
											},
											"formatSettings": {
												"type": "DelimitedTextReadSettings"
											}
										},
										"sink": {
											"type": "AzureSqlSink",
											"writeBehavior": "insert",
											"sqlWriterUseTableLock": false
										},
										"enableStaging": false,
										"translator": {
											"type": "TabularTranslator",
											"typeConversion": true,
											"typeConversionSettings": {
												"allowDataTruncation": true,
												"treatBooleanAsNumber": false
											}
										}
									},
									"inputs": [
										{
											"referenceName": "MultipleFileSource",
											"type": "DatasetReference",
											"parameters": {
												"folder": {
													"value": "@item().sourceFolder",
													"type": "Expression"
												},
												"file": {
													"value": "@item().sourceFile",
													"type": "Expression"
												}
											}
										}
									],
									"outputs": [
										{
											"referenceName": "MultiFileLoadDestination",
											"type": "DatasetReference",
											"parameters": {
												"schema": {
													"value": "@item().destSchema",
													"type": "Expression"
												},
												"table": {
													"value": "@item().desttable",
													"type": "Expression"
												}
											}
										}
									]
								}
							]
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"annotations": [],
				"lastPublishTime": "2022-11-09T10:21:15Z"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/8 Switch Pipeline')]",
			"type": "Microsoft.DataFactory/factories/pipelines",
			"apiVersion": "2018-06-01",
			"properties": {
				"activities": [
					{
						"name": "Switch1",
						"type": "Switch",
						"dependsOn": [],
						"userProperties": [],
						"typeProperties": {
							"on": {
								"value": "@pipeline().parameters.MyChoice",
								"type": "Expression"
							},
							"cases": [
								{
									"value": "delete",
									"activities": [
										{
											"name": "Delete1",
											"type": "Delete",
											"dependsOn": [],
											"policy": {
												"timeout": "0.12:00:00",
												"retry": 0,
												"retryIntervalInSeconds": 30,
												"secureOutput": false,
												"secureInput": false
											},
											"userProperties": [],
											"typeProperties": {
												"dataset": {
													"referenceName": "SwitchDeletingDestination",
													"type": "DatasetReference",
													"parameters": {}
												},
												"enableLogging": false,
												"storeSettings": {
													"type": "AzureBlobFSReadSettings",
													"recursive": true,
													"enablePartitionDiscovery": false
												}
											}
										}
									]
								},
								{
									"value": "set",
									"activities": [
										{
											"name": "Set variable1",
											"type": "SetVariable",
											"dependsOn": [],
											"userProperties": [],
											"typeProperties": {
												"variableName": "Status",
												"value": "complete"
											}
										},
										{
											"name": "Append variable2",
											"type": "AppendVariable",
											"dependsOn": [
												{
													"activity": "Set variable1",
													"dependencyConditions": [
														"Succeeded"
													]
												}
											],
											"userProperties": [],
											"typeProperties": {
												"variableName": "Status Log",
												"value": {
													"value": "@variables('Status')",
													"type": "Expression"
												}
											}
										}
									]
								}
							],
							"defaultActivities": [
								{
									"name": "Wait1",
									"type": "Wait",
									"dependsOn": [],
									"userProperties": [],
									"typeProperties": {
										"waitTimeInSeconds": 5
									}
								}
							]
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"parameters": {
					"MyChoice": {
						"type": "string"
					}
				},
				"variables": {
					"Status": {
						"type": "String",
						"defaultValue": "Process"
					},
					"Status Log": {
						"type": "Array",
						"defaultValue": [
							"Process"
						]
					}
				},
				"annotations": [],
				"lastPublishTime": "2022-11-09T10:21:15Z"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/AlterTable dataflow')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "UpsertSource",
								"type": "DatasetReference"
							},
							"name": "source1"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "UpsertDestination",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "alterRow1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          empId as short,",
						"          empName as string,",
						"          gender as string,",
						"          deptid as short",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> source1",
						"source1 alterRow(upsertIf(true())) ~> alterRow1",
						"alterRow1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          empId as integer,",
						"          empName as string,",
						"          gender as string,",
						"          deptid as integer",
						"     ),",
						"     deletable:false,",
						"     insertable:false,",
						"     updateable:false,",
						"     upsertable:true,",
						"     keys:['empId'],",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError') ~> sink1"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('factoryId'), '/datasets/UpsertSource')]"
			]
		},
		{
			"name": "[concat(parameters('factoryName'), '/DuplicateRemoving Dataflow')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "EmpFilewithDuplicate",
								"type": "DatasetReference"
							},
							"name": "source1"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "EmpDestination",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "aggregate1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          empid as short,",
						"          empname as string,",
						"          age as short,",
						"          gender as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> source1",
						"source1 aggregate(groupBy(empId = empid),",
						"     each(match(name!='empid'), $$ = first($$))) ~> aggregate1",
						"aggregate1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['FinalEmpDetails.csv'],",
						"     umask: 0022,",
						"     preCommands: [],",
						"     postCommands: [],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/Exist_Transformation7')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "Exist_Source2",
								"type": "DatasetReference"
							},
							"name": "source1"
						},
						{
							"dataset": {
								"referenceName": "Exist_Source1",
								"type": "DatasetReference"
							},
							"name": "source2"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "Exist_Destination",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "exists1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          cusId as short,",
						"          cusName as string,",
						"          gender as string,",
						"          netpurchaseAmount as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> source1",
						"source(output(",
						"          cusId as short,",
						"          cusName as string,",
						"          gender as string,",
						"          netpurchaseAmount as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> source2",
						"source1, source2 exists(source1@cusId == source2@cusId,",
						"     negate:true,",
						"     broadcast: 'auto')~> exists1",
						"exists1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          item as string,",
						"          {2018} as string,",
						"          {2019} as string,",
						"          {S.No} as string",
						"     ),",
						"     partitionFileNames:['Exist_Output.csv'],",
						"     umask: 0022,",
						"     preCommands: [],",
						"     postCommands: [],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/Flatten_Transformation5')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "Flatten_Transformation5_Source",
								"type": "DatasetReference"
							},
							"name": "source1"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "Flatten_Transformation5_Destination",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "flatten1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          contact as (landline as integer, mobile as integer),",
						"          name as string,",
						"          skills as string[]",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false,",
						"     documentForm: 'documentPerLine') ~> source1",
						"source1 foldDown(unroll(skills),",
						"     mapColumn(",
						"          Name = name,",
						"          Skills = skills,",
						"          Mobile = contact.mobile,",
						"          Landline = contact.landline",
						"     ),",
						"     skipDuplicateMapInputs: false,",
						"     skipDuplicateMapOutputs: false) ~> flatten1",
						"flatten1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     umask: 0022,",
						"     preCommands: [],",
						"     postCommands: [],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/Joint_Transformation8')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "Joint_Source1",
								"type": "DatasetReference"
							},
							"name": "source1"
						},
						{
							"dataset": {
								"referenceName": "Joint_Source2",
								"type": "DatasetReference"
							},
							"name": "source2"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "Joint_Destination",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "join1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          passId as string,",
						"          passName as string,",
						"          Age as string,",
						"          travalId as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> source1",
						"source(output(",
						"          travalId as string,",
						"          arrivalTime as string,",
						"          departureTime as string,",
						"          travalTime as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> source2",
						"source1, source2 join(source1@travalId == source2@travalId,",
						"     joinType:'left',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> join1",
						"join1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          cusId as string,",
						"          cusName as string,",
						"          gender as string,",
						"          netpurchaseAmount as string",
						"     ),",
						"     partitionFileNames:['joint_Output.csv'],",
						"     umask: 0022,",
						"     preCommands: [],",
						"     postCommands: [],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/MergeMultiRow Dataflow')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "StudentskillsSource",
								"type": "DatasetReference"
							},
							"name": "source1"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "StudentSkillDestination",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "aggregate1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          stuId as short,",
						"          stuName as string,",
						"          skills as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> source1",
						"source1 aggregate(groupBy(stuId,",
						"          stuName),",
						"     skills = replace(replace(replace(toString(collect(skills)),'[',''),'\"',''),']','')) ~> aggregate1",
						"aggregate1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['FinalStudentSkill.csv'],",
						"     umask: 0022,",
						"     preCommands: [],",
						"     postCommands: [],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/Parse_Transformation4')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "Parse_Transformation4_SQLSource",
								"type": "DatasetReference"
							},
							"name": "source1"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "Parse_Transformation4_CSVDestination",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "parse1"
						},
						{
							"name": "parse2"
						}
					],
					"scriptLines": [
						"source(output(",
						"          stuId as integer,",
						"          stuName as string,",
						"          subjects as string,",
						"          location as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     isolationLevel: 'READ_UNCOMMITTED',",
						"     format: 'table') ~> source1",
						"source1 parse(subjectsCol = subjects ? (Subject1 as string,",
						"          Subject2 as string),",
						"     format: 'delimited',",
						"     columnNamesAsHeader: false,",
						"     columnDelimiter: ',',",
						"     nullValue: '') ~> parse1",
						"parse1 parse(locationcol = location ? (City as string,",
						"          State as string),",
						"     format: 'json',",
						"     documentForm: 'singleDocument') ~> parse2",
						"parse2 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['StudentSubject.csv'],",
						"     umask: 0022,",
						"     preCommands: [],",
						"     postCommands: [],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     mapColumn(",
						"          StudentId = stuId,",
						"          StudentName = stuName,",
						"          Subject1 = subjectsCol.Subject1,",
						"          Subject2 = subjectsCol.Subject2,",
						"          City = locationcol.City,",
						"          State = locationcol.State",
						"     ),",
						"     partitionBy('hash', 1)) ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/Pivot_Transformation6')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "Pivot_Transformation6_Source",
								"type": "DatasetReference"
							},
							"name": "source1"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "Pivot_Transformation6_Destination",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "pivot1"
						},
						{
							"name": "surrogateKey1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          stockId as short,",
						"          item as string,",
						"          Quantity as short,",
						"          year as short",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> source1",
						"source1 pivot(groupBy(item),",
						"     pivotBy(year),",
						"     {} = sum(Quantity),",
						"     columnNaming: '$N$V',",
						"     lateral: true) ~> pivot1",
						"pivot1 keyGenerate(output({S.No} as long),",
						"     startAt: 1L,",
						"     stepValue: 1L) ~> surrogateKey1",
						"surrogateKey1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['Pivot_Output'],",
						"     umask: 0022,",
						"     preCommands: [],",
						"     postCommands: [],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/Transfomation1')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"description": "In this Transformation to implementing Rank, Conditional split, derived column, union, sort",
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "Transformation1_Source",
								"type": "DatasetReference"
							},
							"name": "source1"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "Transformation1_Destination",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "split1"
						},
						{
							"name": "rank1"
						},
						{
							"name": "derivedColumn1"
						},
						{
							"name": "derivedColumn2"
						},
						{
							"name": "union1"
						},
						{
							"name": "sort1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          cusId as short,",
						"          cusName as string,",
						"          gender as string,",
						"          netpurchaseAmount as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> source1",
						"rank1 split(netpurchaseAmount>25000,",
						"     disjoint: false) ~> split1@(HighestPurchase, LowestPurchase)",
						"source1 rank(desc(netpurchaseAmount, true),",
						"     output(PurchasedRank as long)) ~> rank1",
						"split1@HighestPurchase derive({Special Points} = round((netpurchaseAmount/100)*6)) ~> derivedColumn1",
						"split1@LowestPurchase derive({Special Points} = round((netpurchaseAmount/100)*4)) ~> derivedColumn2",
						"derivedColumn1, derivedColumn2 union(byName: true)~> union1",
						"union1 sort(asc(cusId, true)) ~> sort1",
						"sort1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          cusId as string,",
						"          cusName as string,",
						"          gender as string,",
						"          netpurchaseAmount as string",
						"     ),",
						"     umask: 0022,",
						"     preCommands: [],",
						"     postCommands: [],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/Transfomation2')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "Transformation2_EmpSource",
								"type": "DatasetReference"
							},
							"name": "source1"
						},
						{
							"dataset": {
								"referenceName": "Transformation2_DeptSource",
								"type": "DatasetReference"
							},
							"name": "source2"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "Transformation2_Destination",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "assert1"
						},
						{
							"name": "derivedColumn1"
						},
						{
							"name": "filter1"
						},
						{
							"name": "select1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          EmpId as short,",
						"          EmpName as string,",
						"          gender as string,",
						"          DateOfJoin as string,",
						"          deptId as short",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> source1",
						"source(output(",
						"          deptId as short,",
						"          deptName as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> source2",
						"source1, source2 assert(expectTrue(!isNull(toDate(DateOfJoin,'dd-MM-yyyy')), false, 'DateFormat'),",
						"     expectUnique(EmpId, false, 'UniqueId'),",
						"     expectExists(source1@deptId == source2@deptId, false, 'CheckExist')) ~> assert1",
						"assert1 derive({DOJ Error} = hasError('DateFormat'),",
						"          {EmpId Error} = hasError('UniqueId'),",
						"          {DeptIdExist Error} = hasError('CheckExist')) ~> derivedColumn1",
						"derivedColumn1 filter(isError()==false()) ~> filter1",
						"filter1 select(mapColumn(",
						"          EmpId,",
						"          EmpName,",
						"          Gender = gender,",
						"          DateOfJoin",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select1",
						"select1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['ErrorClearedEmployee.Csv'],",
						"     umask: 0022,",
						"     preCommands: [],",
						"     postCommands: [],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/Transformation3')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "Transformation3_Source",
								"type": "DatasetReference"
							},
							"name": "source1"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "Transformation3_Destination",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "select1"
						},
						{
							"name": "select2"
						},
						{
							"name": "derivedColumn1"
						},
						{
							"name": "derivedColumn2"
						},
						{
							"name": "join1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          empId as short,",
						"          empName as string,",
						"          age as short,",
						"          salary as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> source1",
						"source1 select(mapColumn(",
						"          EmpId = empId,",
						"          EmpName = empName,",
						"          Age = age",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select1",
						"source1 select(mapColumn(",
						"          EmpId = empId,",
						"          Salary = salary",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select2",
						"select2 derive(PF = Salary*0.08,",
						"          Tax = Salary*0.05) ~> derivedColumn1",
						"derivedColumn1 derive(NetSalary = Salary-(PF+Tax)) ~> derivedColumn2",
						"derivedColumn2, select1 join(select2@EmpId == select1@EmpId,",
						"     joinType:'inner',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> join1",
						"join1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['EmpPayslip.csv'],",
						"     umask: 0022,",
						"     preCommands: [],",
						"     postCommands: [],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     mapColumn(",
						"          EmpId = select2@EmpId,",
						"          EmpName,",
						"          Age,",
						"          Salary,",
						"          PF,",
						"          Tax,",
						"          NetSalary,",
						"          EmpId = select1@EmpId",
						"     ),",
						"     partitionBy('hash', 1)) ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/Unpivot_Transfomation8')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "Unpivot_Source",
								"type": "DatasetReference"
							},
							"name": "source1"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "Unpivot_Destination",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "unpivot1"
						},
						{
							"name": "cast1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          item as string,",
						"          {2018} as string,",
						"          {2019} as string,",
						"          {S.No} as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> source1",
						"source1 unpivot(output(",
						"          Years as short,",
						"          Quantity as string",
						"     ),",
						"     ungroupBy({S.No},",
						"          item),",
						"     lateral: true,",
						"     ignoreNullPivots: false) ~> unpivot1",
						"unpivot1 cast(output(",
						"          {S.No} as short",
						"     ),",
						"     errors: true) ~> cast1",
						"cast1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          cusId as string,",
						"          cusName as string,",
						"          gender as string,",
						"          netpurchaseAmount as string",
						"     ),",
						"     partitionFileNames:['Unpivot_Output.csv'],",
						"     umask: 0022,",
						"     preCommands: [],",
						"     postCommands: [],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/1 UpdateIfExist InsertIfNew')]",
			"type": "Microsoft.DataFactory/factories/pipelines",
			"apiVersion": "2018-06-01",
			"properties": {
				"description": "My source is file & destination is azure SQL database. if any new rows mean insert the data,\nif that row already presents then update that row.",
				"activities": [
					{
						"name": "Data flow1",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "AlterTable dataflow",
								"type": "DataFlowReference",
								"parameters": {},
								"datasetParameters": {
									"source1": {},
									"sink1": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"annotations": [],
				"lastPublishTime": "2022-11-08T06:26:31Z"
			},
			"dependsOn": [
				"[concat(variables('factoryId'), '/dataflows/AlterTable dataflow')]"
			]
		},
		{
			"name": "[concat(parameters('factoryName'), '/10 Transformation1 Pipeline')]",
			"type": "Microsoft.DataFactory/factories/pipelines",
			"apiVersion": "2018-06-01",
			"properties": {
				"activities": [
					{
						"name": "Data flow1",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "Transfomation1",
								"type": "DataFlowReference",
								"parameters": {},
								"datasetParameters": {
									"source1": {},
									"sink1": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"annotations": [],
				"lastPublishTime": "2022-11-08T12:27:53Z"
			},
			"dependsOn": [
				"[concat(variables('factoryId'), '/dataflows/Transfomation1')]"
			]
		}
	]
}